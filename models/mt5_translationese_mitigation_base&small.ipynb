{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3cda7513-db89-47ba-a348-a9af842097a1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3cda7513-db89-47ba-a348-a9af842097a1",
    "outputId": "88763a28-bb53-4b4b-8fdd-67c99964a14a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
    
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install nltk\n",
    "import nltk\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3094142e-655d-43d6-84ee-488b12ec6658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
 
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install accelerate\n",
    "!pip install hf_xet\n",
    "!pip install datasets\n",
    "!pip install bert-score\n",
    "!pip install blobfile tiktoken\n",
    "!pip install torch==2.3.1 --index-url https://download.pytorch.org/whl/cu121\n",
    "!pip install datasets==3.6.0\n",
    "!pip install tokenizers==0.14.1 \n",
    "!pip install huggingface_hub==0.32.0\n",
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e406c32-b357-4022-88b4-130a9407b1bd",
   "metadata": {
    "id": "1e406c32-b357-4022-88b4-130a9407b1bd"
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "shutil.rmtree(\"/root/.cache/huggingface/metrics\", ignore_errors=True)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5ece5e40-379f-4485-89e7-d25635c6c747",
   "metadata": {
    "id": "5ece5e40-379f-4485-89e7-d25635c6c747"
   },
   "source": [
    "학습이 안되었던 이유\n",
    "T5 및 mT5 계열 모델은 Transformer decoder의 내부에서 use_cache=True가 활성화되어 있\n",
    "으면 학습 중 decoder_input_ids shift가 제대로 동작하지 않아서, labels와의 alignment가 깨짐.\n",
    "허깅페이스에서도 명시되어 있음.\n",
    " T5-like models must have use_cache=False during training, or loss will not be computed correctly\n",
    "\n",
    "model.config.use_cache = False\n",
    "model.config.decoder_start_token_id = tokenizer.eos_token_id\n",
    "으로 해결함!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1fdfe100-91b5-4b5b-a2f8-718605488ee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
   
     ]
    }
   ],
   "source": [
    "!pip install --upgrade torch --index-url https://download.pytorch.org/whl/cu121\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38be497c-2986-4875-990b-4b0f4834aeaa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "3db0691c2149494a954b4ed993fc9707",
      "c15f82108f424c15a5841b9255a623f0",
      "fe638bb582bc4fbf8c36e2deff1a4e2d",
      "b1dea5ee24944a9aa4dbd0cd7f39008b",
      "f7c24c3d6b4644f8ad3d103dd9c75ce1",
      "214f7b7c45e84dc8916f31988e1039eb",
      "71be0ce8f4c641378c799fc231abe6dd",
      "0a73f471607d4cc8b66a14914c7ebb34",
      "2370eb6bdaf646589f09aa75d1bf58ec",
      "908b8c7f74a34fb598a85ddb26f6e98c",
      "608b9c94c523414782b09b5284aef51f",
      "51e221d6cecd49e1bf744269f05ec430",
      "86df0975f00648819dde178a1864bcd6",
      "42683c31707c4b5a9bb8aae95870881f",
      "d497d8842a6040ed9d57f29465afad9a",
      "e78bd0649de24106b06ff37f04942c0f",
      "56fffba355ab4331b62e1af32b65db2b",
      "10046844076b4923856f236029564a89",
      "289192d1a6cd45b5a13c374ddd87984b",
      "7d23c5b6a3024ffcb2197740bdedb1a8",
      "85912ace6ed1420397426adf5f44cf7b",
      "69b3e7d7fd3649b4986917204fb38ce3"
     ]
    },
    "id": "38be497c-2986-4875-990b-4b0f4834aeaa",
    "outputId": "a3500f07-cf97-45a7-cb47-d75982476c42"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Hardware Info]\n",
      "CUDA Available: True\n",
      "Device Name: NVIDIA RTX A6000\n",
      "Memory Allocated (MB): 2227.1845703125\n",
      "Memory Cached (MB): 2290.0\n",
      "Total Memory (MB): 48550.4375\n",
      "CPU Cores: 20\n",
      "Logical CPUs: 40\n",
      "Total RAM (GB): 188.54\n",
      "Cleaning GPU Cache...\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca66e69d41134af3aa230e6645c77f39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85593 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "583ad6583c054f4bb3f334d8e04d3d6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/21399 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSeq2SeqLM,\n",
    "    DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    ")\n",
    "from evaluate import load\n",
    "from nltk.tokenize import word_tokenize\n",
    "import editdistance\n",
    "import torch.nn.functional as F\n",
    "import psutil\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:32\"\n",
    "\n",
    "#  하드웨어 정보 출력 및 캐시 정리\n",
    "def show_hardware_status():\n",
    "    print(\"\\n[Hardware Info]\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "        print(\"Device Name:\", torch.cuda.get_device_name(0))\n",
    "        print(\"Memory Allocated (MB):\", torch.cuda.memory_allocated(0) / 1024 ** 2)\n",
    "        print(\"Memory Cached (MB):\", torch.cuda.memory_reserved(0) / 1024 ** 2)\n",
    "        print(\"Total Memory (MB):\", torch.cuda.get_device_properties(0).total_memory / 1024 ** 2)\n",
    "    else:\n",
    "        print(\"CUDA not available. Using CPU.\")\n",
    "    print(\"CPU Cores:\", psutil.cpu_count(logical=False))\n",
    "    print(\"Logical CPUs:\", psutil.cpu_count(logical=True))\n",
    "    print(\"Total RAM (GB):\", round(psutil.virtual_memory().total / 1024**3, 2))\n",
    "    print(\"Cleaning GPU Cache...\\n\")\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "show_hardware_status()\n",
    "\n",
    "# Model/tokenizer load (base or small)\n",
    "model_name = \"google/mt5-small\"  # mT5-small/mt5-base\" \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# T5 계열 학습 시 필수 설정\n",
    "model.config.use_cache = False\n",
    "model.config.decoder_start_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# Full-data loading\n",
    "\n",
    "raw_df = pd.read_csv(\"/home/itbecomesteam/yai/Data_filtered.csv\", encoding=\"utf-8\")\n",
    "# raw_df = raw_df[[\"source\", \"ko\"]].rename(columns={\"source\": \"input_text\", \"ko\": \"target_text\"})\n",
    "raw_df = raw_df[[\"ko_translationese\", \"ko\"]].rename(columns={\"ko_translationese\": \"input_text\", \"ko\": \"target_text\"})\n",
    "raw_df = raw_df.dropna()\n",
    "raw_df[\"input_text\"] = raw_df[\"input_text\"].astype(str)\n",
    "raw_df[\"target_text\"] = raw_df[\"target_text\"].astype(str)\n",
    "\n",
    "raw_df = raw_df[~raw_df[\"input_text\"].str.strip().eq(\"\")]\n",
    "raw_df = raw_df[~raw_df[\"target_text\"].str.strip().eq(\"\")]\n",
    "\n",
    "raw_dataset = Dataset.from_pandas(raw_df)\n",
    "dataset_split = raw_dataset.train_test_split(test_size=0.2, seed=42)\n",
    "full_ds = DatasetDict({\n",
    "    \"train\": dataset_split[\"train\"],\n",
    "    \"validation\": dataset_split[\"test\"]\n",
    "})\n",
    "\n",
    "# Preprocessing\n",
    "def preprocess(example):\n",
    "    model_inputs = tokenizer(\n",
    "        example[\"input_text\"], max_length=128, padding=\"max_length\", truncation=True\n",
    "    )\n",
    "    labels = tokenizer(\n",
    "        text_target=example[\"target_text\"], max_length=128, padding=\"max_length\", truncation=True\n",
    "    )[\"input_ids\"]\n",
    "    labels = [label if label != tokenizer.pad_token_id else -100 for label in labels]\n",
    "    model_inputs[\"labels\"] = labels\n",
    "    return model_inputs\n",
    "\n",
    "full_tokenized = full_ds.map(preprocess, remove_columns=full_ds[\"train\"].column_names)\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, padding=True)\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    full_tokenized[\"validation\"],\n",
    "    batch_size=8,\n",
    "    collate_fn=data_collator\n",
    ")\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def compute_perplexity_from_preds(preds, refs):\n",
    "    total_logp = 0.0\n",
    "    total_tokens = 0\n",
    "\n",
    "    for pred_text, ref_text in zip(preds, refs):\n",
    "        input_ids = tokenizer(pred_text, return_tensors=\"pt\", padding=True, truncation=True).input_ids.to(model.device)\n",
    "        labels = tokenizer(ref_text, return_tensors=\"pt\", padding=True, truncation=True).input_ids.to(model.device)\n",
    "        with torch.no_grad():\n",
    "            output = model(input_ids=input_ids, labels=labels)\n",
    "        loss = output.loss\n",
    "        n_tokens = (labels != -100).sum().item()\n",
    "        total_logp += loss.item() * n_tokens\n",
    "        total_tokens += n_tokens\n",
    "\n",
    "    return math.exp(total_logp / total_tokens) if total_tokens > 0 else float(\"inf\")\n",
    "\n",
    "\n",
    "# Matric\n",
    "def compute_metrics(eval_preds):\n",
    "    pred_ids = eval_preds.predictions\n",
    "    label_ids = eval_preds.label_ids\n",
    "\n",
    "    # padding 토큰 처리\n",
    "    pred_ids = np.where(pred_ids != -100, pred_ids, tokenizer.pad_token_id)\n",
    "    label_ids = np.where(label_ids != -100, label_ids, tokenizer.pad_token_id)\n",
    "\n",
    "    # 디코딩\n",
    "    preds = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    refs = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "    preds = [p.strip() for p in preds]\n",
    "    refs = [r.strip() for r in refs]\n",
    "\n",
    "    preds_nonempty = [p for p in preds if p]\n",
    "    refs_nonempty = [r for r in refs if r]\n",
    "\n",
    "    if not preds_nonempty or not refs_nonempty:\n",
    "        return {\n",
    "            \"MATTR\": 0.0,\n",
    "            \"KoBERTScore_F1\": 0.0,\n",
    "            \"Perplexity\": float(\"inf\"),\n",
    "            \"Levenshtein_Distance\": float(\"inf\")\n",
    "        }\n",
    "\n",
    "    # MATTR\n",
    "    def mattr(texts, window_size=50):\n",
    "        def single(text):\n",
    "            tokens = text.split()\n",
    "            if len(tokens) < window_size:\n",
    "                return len(set(tokens)) / len(tokens) if tokens else 0.0\n",
    "            return np.mean([\n",
    "                len(set(tokens[i:i+window_size])) / window_size\n",
    "                for i in range(len(tokens) - window_size + 1)\n",
    "            ])\n",
    "        return np.mean([single(t) for t in texts])\n",
    "\n",
    "    mattr_score = mattr(preds_nonempty)\n",
    "\n",
    "    # BERTScore (KoBERT)\n",
    "    bs_metric = load(\"bertscore\", keep_in_memory=True)\n",
    "    bs_results = bs_metric.compute(predictions=preds_nonempty, references=refs_nonempty, lang=\"ko\")\n",
    "    f1_kobert = float(np.mean(bs_results[\"f1\"]))\n",
    "\n",
    "    # Levenshtein Distance\n",
    "    import editdistance\n",
    "    ld = np.mean([editdistance.eval(p, r) for p, r in zip(preds_nonempty, refs_nonempty)])\n",
    "\n",
    "    # PPL: preds + refs \n",
    "    def compute_perplexity_from_preds(preds, refs):\n",
    "        model.eval()\n",
    "        total_logp = 0.0\n",
    "        total_tokens = 0\n",
    "\n",
    "        for pred_text, ref_text in zip(preds, refs):\n",
    "            input_ids = tokenizer(pred_text, return_tensors=\"pt\", padding=True, truncation=True).input_ids.to(model.device)\n",
    "            labels = tokenizer(ref_text, return_tensors=\"pt\", padding=True, truncation=True).input_ids.to(model.device)\n",
    "            labels[labels == tokenizer.pad_token_id] = -100\n",
    "            with torch.no_grad():\n",
    "                outputs = model(input_ids=input_ids, labels=labels)\n",
    "                loss = outputs.loss\n",
    "            n_tokens = (labels != -100).sum().item()\n",
    "            total_logp += loss.item() * n_tokens\n",
    "            total_tokens += n_tokens\n",
    "\n",
    "        return math.exp(total_logp / total_tokens) if total_tokens > 0 else float(\"inf\")\n",
    "\n",
    "    ppl = compute_perplexity_from_preds(preds_nonempty, refs_nonempty)\n",
    "\n",
    "    return {\n",
    "        \"MATTR\": round(mattr_score, 4),\n",
    "        \"KoBERTScore_F1\": round(f1_kobert, 4),\n",
    "        \"Perplexity\": round(ppl, 4),\n",
    "        \"Levenshtein_Distance\": round(ld, 2),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8330bb23-e033-4fc8-a13b-682bab7bb9c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
     ]
    }
   ],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    label_pad_token_id=-100 \n",
    ")\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"output/mt5_small-translationese-mitigation\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=100,\n",
    "    learning_rate=3e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=2,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=64,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    fp16=True,  \n",
    "    report_to=[\"wandb\", \"tensorboard\"]\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=full_tokenized[\"train\"],\n",
    "    eval_dataset=full_tokenized[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "90087d18-d05b-4914-a903-bcd3947f2c09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1+cu121\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)  #  최소 2.2 이상이어야 함\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2400d821-e09b-41bf-bf34-bdbc42654b7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
  
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
   
      ],
      "text/plain": [
 
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
     
      ],
      "text/plain": [
       
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
      
      ],
      "text/plain": [
      
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       
      ],
      "text/plain": [
       
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
  
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13375' max='13375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13375/13375 5:15:10, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Mattr</th>\n",
       "      <th>Kobertscore F1</th>\n",
       "      <th>Perplexity</th>\n",
       "      <th>Levenshtein Distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.608600</td>\n",
       "      <td>2.058083</td>\n",
       "      <td>0.992200</td>\n",
       "      <td>0.853800</td>\n",
       "      <td>9.215000</td>\n",
       "      <td>15.590000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.336800</td>\n",
       "      <td>1.941612</td>\n",
       "      <td>0.993500</td>\n",
       "      <td>0.856700</td>\n",
       "      <td>7.968300</td>\n",
       "      <td>15.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.247700</td>\n",
       "      <td>1.874399</td>\n",
       "      <td>0.993900</td>\n",
       "      <td>0.858000</td>\n",
       "      <td>7.392200</td>\n",
       "      <td>15.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.193000</td>\n",
       "      <td>1.846628</td>\n",
       "      <td>0.993500</td>\n",
       "      <td>0.858700</td>\n",
       "      <td>7.138100</td>\n",
       "      <td>15.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.214000</td>\n",
       "      <td>1.839588</td>\n",
       "      <td>0.993600</td>\n",
       "      <td>0.858800</td>\n",
       "      <td>7.098200</td>\n",
       "      <td>15.030000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
   
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=13375, training_loss=2.6290146393107476, metrics={'train_runtime': 18914.4208, 'train_samples_per_second': 22.626, 'train_steps_per_second': 0.707, 'total_flos': 5.65715903643648e+16, 'train_loss': 2.6290146393107476, 'epoch': 5.0})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cc931625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best checkpoint path: output/mt5_small-translationese-mitigation/checkpoint-13375\n"
     ]
    }
   ],
   "source": [
    "best_ckpt = trainer.state.best_model_checkpoint\n",
    "print(\"Best checkpoint path:\", best_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0940d274-a436-4459-8916-8eb7e0be74d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              input_text  \\\n",
      "14653            다양한 보고서 양식은 무료일 뿐만 아니라 자유롭게 편집할 수 있습니다.   \n",
      "74244              이건 당신에게 아주 새로운 음식일 수도 있지만 추천해 드리겠습니다.   \n",
      "70677                        이 프로젝트에 헌신한 시간과 노력에 감사드립니다.   \n",
      "5693                         당신과 당신의 친구들에게 초대장을 보내야 합니다!   \n",
      "94545  \"내가 챔피언이 되지 않더라도, 나는 이미 당신의 참석으로 챔피언이 됐다\"고 그녀는...   \n",
      "\n",
      "                                            target_text  \n",
      "14653    다양한 보고서 양식이 무료 제공될 뿐만 아니라 자유롭게 양식을 편집할 수 있습니다.  \n",
      "74244                 귀하께는 아주 새로운 음식일지 모르나 추천 드리고 싶습니다.  \n",
      "70677                         이 프로젝트 진행을 위한 노고에 감사드립니다.  \n",
      "5693                            여러분과 여러분의 친구들을 초대하겠습니다!  \n",
      "94545  \"챔피언이 되지 못하더라도, 이미 엄마의 참석으로 챔피언이에요\"라고 그녀가 말했습니다.  \n"
     ]
    }
   ],
   "source": [
    "print(raw_df.sample(5))  # input_text와 target_text가 비정상적으로 동일하거나 빈 값이 있는지 확인\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8eb7d42-67c8-4a4f-8d73-419957d5ae9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 입력: UV 보호 기능을 가지고 있습니다. 400mm까지의 모든 유해한 파란색 빛을 100% 필터링합니다. 하지만 착용하기에 매우 편리합니다.\n",
      "✅ 출력: UV 보호 기능은 400mm까지의 모든 유해한 파란색 빛을 100% 필터링합니다.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "model_path = \"output/mt5_small-translationese-mitigation/checkpoint-13375\"  \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_path).to(\"cuda\")\n",
    "\n",
    "# 샘플\n",
    "input_text = \"UV 보호 기능을 가지고 있습니다. 400mm까지의 모든 유해한 파란색 빛을 100% 필터링합니다. 하지만 착용하기에 매우 편리합니다.\"\n",
    "\n",
    "# inference\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=128).to(\"cuda\")\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(**inputs,decoder_start_token_id=tokenizer.eos_token_id, max_new_tokens=256, num_beams=5)\n",
    "result = tokenizer.decode(outputs[0], skip_special_tokens=True).replace(\"<extra_id_0>\", \"\").strip()\n",
    "\n",
    "print(\"✅ 입력:\", input_text)\n",
    "print(\"✅ 출력:\", result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "88ca57c6-b54d-470d-b9ab-6a751c1bcce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 1: 철수는 10개의 소를 키운다.\n",
      "완화 1: 철수는 10개의 소를 키운다.\n",
      "\n",
      "입력 2: 그 아이들은 보육원에 의해 보호되었다.\n",
      "완화 2: 그 아이들은 보육원에 의해 보호되었습니다.\n",
      "\n",
      "입력 3: 전통적인 중국 문화에서 결혼에 대한 결정들은 그들의 자녀를 위하는 부모들에 의해 만들어졌다.\n",
      "완화 3: 전통적인 중국 문화에서 결혼에 대한 결정은 그들의 자녀를 위하는 부모들에 의해 만들어졌습니다.\n",
      "\n",
      "입력 4: 다음 물음에 대해 답하시오.\n",
      "완화 4: 다음 물음에 대해 답변해 주십시오.\n",
      "\n",
      "입력 5: 고모는 두 명을 아들을 가지고 있다.\n",
      "완화 5: 고모는 두 명을 가지고 있어요.\n",
      "\n",
      "입력 6: 나는 미국 여행을 가기 위해 영어를 공부했다.\n",
      "완화 6: 나는 미국 여행을 가기 위해 영어를 공부했습니다.\n",
      "\n",
      "입력 7: 학생들은 공부함에 있어서 불편함이 없었다.\n",
      "완화 7: 학생들은 공부함에 있어서 불편함이 없었습니다.\n",
      "\n",
      "입력 8: 성공하기 위해서는 열심히 노력하지 않으면 안 된다.\n",
      "완화 8: 성공하기 위해서는 열심히 노력하지 않으면 안 됩니다.\n",
      "\n",
      "입력 9: 그들 대부분은 좋은 직장을 얻는 데는 새로운 아이디어들과 방법들이 있다는 것을 모른다.\n",
      "완화 9: 그들 대부분은 좋은 직장을 얻는 데 새로운 아이디어와 방법이 있다는 것을 모른다.\n",
      "\n",
      "입력 10: 두 사람이 주점의 2층에서의 살림을 그만두고 교외로 이사하여 노라의 아기 주디와 살고 있다는 이야기를 들었다.\n",
      "완화 10: 두 사람이 주점의 2층에서 살림을 그만두고 교외로 이사하여 노라의 아기 주디와 살고 있다는 이야기를 들었습니다.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "\n",
    "# 모델 로딩\n",
    "model_dir = \"output/mt5_small-translationese-mitigation/checkpoint-13375\" \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_dir).to(\"cuda\")\n",
    "model.eval()\n",
    "\n",
    "translationese_list = [\n",
    "           '철수는 10개의 소를 키운다.',\n",
    "           '그 아이들은 보육원에 의해 보호되었다.', #보육원은 그 아이들을 보호했다.\n",
    "           '전통적인 중국 문화에서 결혼에 대한 결정들은 그들의 자녀를 위하는 부모들에 의해 만들어졌다.', #중국의 전통문화로는, 혼인 결정을 자식 대신 부모가 했다.\n",
    "           '다음 물음에 대해 답하시오.', # 다음 물음에 답하시오.\n",
    "           '고모는 두 명을 아들을 가지고 있다.', #고모는 아들이 두 명 있다.\n",
    "           '나는 미국 여행을 가기 위해 영어를 공부했다.', # 나는 미국 여행 가려고 영어를 공부했다.\n",
    "           '학생들은 공부함에 있어서 불편함이 없었다.', # 학생들은 공부하는 데에 불편함이 없었다.\n",
    "           '성공하기 위해서는 열심히 노력하지 않으면 안 된다.', # 성공하려면 열심히 노력해야 한다. \n",
    "           '그들 대부분은 좋은 직장을 얻는 데는 새로운 아이디어들과 방법들이 있다는 것을 모른다.', #그들 가운데 대다수는 좋은 직업을 구하는 데 있어서 여러 가지 남다른 생각과 방법이 있다는 것을 모른다.\n",
    "           '두 사람이 주점의 2층에서의 살림을 그만두고 교외로 이사하여 노라의 아기 주디와 살고 있다는 이야기를 들었다.' # 두 사람이 주점의 2층에서 시작한 살림을 그만두고 교외로 이사하여 노라의 아기 주디와 살고 있다는 이야기를 들었다.\n",
    "]\n",
    "\n",
    "# 완화된 결과 생성\n",
    "for idx, sentence in enumerate(translationese_list, 1):\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=128).to(\"cuda\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            decoder_start_token_id=tokenizer.eos_token_id,\n",
    "            max_new_tokens=128,\n",
    "            num_beams=5,\n",
    "            early_stopping=True\n",
    "        )\n",
    "        decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "        if \"<extra_id_0>\" in decoded:\n",
    "            result = decoded.split(\"<extra_id_0>\", 1)[-1].strip()\n",
    "        else:\n",
    "            result = decoded.strip()\n",
    "        result = result.lstrip(\" .>\")\n",
    "\n",
    "    # result = tokenizer.decode(output[0], skip_special_tokens=True).replace(\"<extra_id_0>\", \"\").strip()\n",
    "\n",
    "    print(f\"입력 {idx}: {sentence}\")\n",
    "    print(f\"완화 {idx}: {result}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
