{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75b7f3d5-4d96-4d4d-8c6a-1b2a17c4a053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
     ]
    }
   ],
   "source": [
    "!pip install transformers datasets sentencepiece --quiet\n",
    "import torch\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from datasets import Dataset, DatasetDict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from evaluate import load\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c92b6276-327d-4e2d-b511-e8e53cdea553",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4098eb30-d4fb-4869-a82b-b2a0d4a9bb60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
    
     ]
    }
   ],
   "source": [
    "!pip install wandb\n",
    "!pip install tensorboard\n",
    "!pip install --upgrade torch torchvision --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install transformers==4.29.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b53d74b-0739-4d70-a746-681e1a8dde96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [

     ]
    }
   ],
   "source": [
    "!pip install safetensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38efcd12-588b-4c73-b863-fd3d926fd0f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
     
     ]
    }
   ],
   "source": [
    "!pip install accelerate\n",
    "!pip install datasets\n",
    "!pip install bert-score\n",
    "!pip install blobfile tiktoken\n",
    "!pip install torch==2.3.1 --index-url https://download.pytorch.org/whl/cu121\n",
    "!pip install datasets==3.6.0\n",
    "!pip install tokenizers==0.14.1 \n",
    "!pip install huggingface_hub==0.32.0\n",
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62e5868c-be63-4a8c-ba91-b0708def32a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_preds):\n",
    "    pred_ids = eval_preds.predictions\n",
    "    label_ids = eval_preds.label_ids\n",
    "\n",
    "    # padding 토큰 처리\n",
    "    pred_ids = np.where(pred_ids != -100, pred_ids, tokenizer.pad_token_id)\n",
    "    label_ids = np.where(label_ids != -100, label_ids, tokenizer.pad_token_id)\n",
    "\n",
    "    # 디코딩\n",
    "    preds = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    refs = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "    preds = [p.strip() for p in preds]\n",
    "    refs = [r.strip() for r in refs]\n",
    "\n",
    "    preds_nonempty = [p for p in preds if p]\n",
    "    refs_nonempty = [r for r in refs if r]\n",
    "\n",
    "    if not preds_nonempty or not refs_nonempty:\n",
    "        return {\n",
    "            \"MATTR\": 0.0,\n",
    "            \"KoBERTScore_F1\": 0.0,\n",
    "            \"Perplexity\": float(\"inf\"),\n",
    "            \"Levenshtein_Distance\": float(\"inf\")\n",
    "        }\n",
    "\n",
    "    # MATTR\n",
    "    def mattr(texts, window_size=50):\n",
    "        def single(text):\n",
    "            tokens = text.split()\n",
    "            if len(tokens) < window_size:\n",
    "                return len(set(tokens)) / len(tokens) if tokens else 0.0\n",
    "            return np.mean([\n",
    "                len(set(tokens[i:i+window_size])) / window_size\n",
    "                for i in range(len(tokens) - window_size + 1)\n",
    "            ])\n",
    "        return np.mean([single(t) for t in texts])\n",
    "\n",
    "    mattr_score = mattr(preds_nonempty)\n",
    "\n",
    "    # BERTScore (KoBERT)\n",
    "    bs_metric = load(\"bertscore\", keep_in_memory=True)\n",
    "    bs_results = bs_metric.compute(predictions=preds_nonempty, references=refs_nonempty, lang=\"ko\")\n",
    "    f1_kobert = float(np.mean(bs_results[\"f1\"]))\n",
    "\n",
    "    # Levenshtein Distance\n",
    "    import editdistance\n",
    "    ld = np.mean([editdistance.eval(p, r) for p, r in zip(preds_nonempty, refs_nonempty)])\n",
    "\n",
    "    # PPL: preds + refs 기반 계산\n",
    "    def compute_perplexity_from_preds(preds, refs):\n",
    "        model.eval()\n",
    "        total_logp = 0.0\n",
    "        total_tokens = 0\n",
    "\n",
    "        for pred_text, ref_text in zip(preds, refs):\n",
    "            input_ids = tokenizer(pred_text, return_tensors=\"pt\", padding=True, truncation=True).input_ids.to(model.device)\n",
    "            labels = tokenizer(ref_text, return_tensors=\"pt\", padding=True, truncation=True).input_ids.to(model.device)\n",
    "            labels[labels == tokenizer.pad_token_id] = -100\n",
    "            with torch.no_grad():\n",
    "                outputs = model(input_ids=input_ids, labels=labels)\n",
    "                loss = outputs.loss\n",
    "            n_tokens = (labels != -100).sum().item()\n",
    "            total_logp += loss.item() * n_tokens\n",
    "            total_tokens += n_tokens\n",
    "\n",
    "        return math.exp(total_logp / total_tokens) if total_tokens > 0 else float(\"inf\")\n",
    "\n",
    "    ppl = compute_perplexity_from_preds(preds_nonempty, refs_nonempty)\n",
    "\n",
    "    return {\n",
    "        \"MATTR\": round(mattr_score, 4),\n",
    "        \"KoBERTScore_F1\": round(f1_kobert, 4),\n",
    "        \"Perplexity\": round(ppl, 4),\n",
    "        \"Levenshtein_Distance\": round(ld, 2),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "377a4cbb-eb2e-4f95-8c02-ade3fbf33540",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
   
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "660b3a1b354246acba770b6013856bc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "071271a82b1f4d85bce3cb399b7337a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import PreTrainedTokenizerFast, BartForConditionalGeneration, Trainer, TrainingArguments, Seq2SeqTrainingArguments, DataCollatorForSeq2Seq\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "model_name = \"gogamza/kobart-base-v2\"\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(model_name)\n",
    "model     = BartForConditionalGeneration.from_pretrained(model_name)\n",
    "df = pd.read_csv(\"Data_filtered.csv\", encoding=\"utf-8\")\n",
    "df = df[[\"ko_translationese\", \"ko\"]].rename(columns={\"ko_translationese\": \"input_text\", \"ko\": \"target_text\"})\n",
    "df = df.dropna()\n",
    "df = df[df[\"input_text\"].str.strip() != \"\"]\n",
    "df = df[df[\"target_text\"].str.strip() != \"\"]\n",
    "\n",
    "dataset = Dataset.from_pandas(df)\n",
    "dataset = dataset.train_test_split(test_size=0.1, seed=42)\n",
    "\n",
    "def preprocess_function(example):\n",
    "    inputs = tokenizer(example[\"input_text\"], max_length=128, padding=\"max_length\", truncation=True)\n",
    "    targets = tokenizer(example[\"target_text\"], max_length=128, padding=\"max_length\", truncation=True)\n",
    "    inputs[\"labels\"] = targets[\"input_ids\"]\n",
    "    return inputs\n",
    "\n",
    "tokenized = dataset.map(preprocess_function, remove_columns=dataset[\"train\"].column_names)\n",
    "\n",
    "from transformers import Seq2SeqTrainingArguments\n",
    "from transformers import PreTrainedTokenizerFast, BartForConditionalGeneration\n",
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments, DataCollatorForSeq2Seq\n",
    "\n",
    "model_name = \"gogamza/kobart-base-v2\"\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(model_name)\n",
    "model     = BartForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./kobart_translationese\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=100,\n",
    "    learning_rate=3e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=2,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "\n",
    "    predict_with_generate=True,       # Seq2Seq 전용\n",
    "    generation_max_length=64,         # Seq2Seq 전용\n",
    "\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    fp16=True,\n",
    "    report_to=[\"wandb\", \"tensorboard\"],\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized[\"train\"],\n",
    "    eval_dataset=tokenized[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95211b5f-e30d-48ab-b1aa-954f56360bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers==4.52.4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0387874d-07b0-42dd-bfb4-286452a5bcea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30090' max='30090' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [30090/30090 1:55:13, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Mattr</th>\n",
       "      <th>Kobertscore F1</th>\n",
       "      <th>Perplexity</th>\n",
       "      <th>Levenshtein Distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.170700</td>\n",
       "      <td>0.186114</td>\n",
       "      <td>0.993500</td>\n",
       "      <td>0.866400</td>\n",
       "      <td>8.341800</td>\n",
       "      <td>14.140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.144600</td>\n",
       "      <td>0.185334</td>\n",
       "      <td>0.993200</td>\n",
       "      <td>0.866200</td>\n",
       "      <td>8.836600</td>\n",
       "      <td>14.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.125300</td>\n",
       "      <td>0.187664</td>\n",
       "      <td>0.993600</td>\n",
       "      <td>0.866300</td>\n",
       "      <td>9.501400</td>\n",
       "      <td>14.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.113500</td>\n",
       "      <td>0.190936</td>\n",
       "      <td>0.993400</td>\n",
       "      <td>0.865700</td>\n",
       "      <td>10.158700</td>\n",
       "      <td>14.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.110500</td>\n",
       "      <td>0.190937</td>\n",
       "      <td>0.993400</td>\n",
       "      <td>0.865700</td>\n",
       "      <td>10.158600</td>\n",
       "      <td>14.300000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [

     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "\n",
    "model.save_pretrained(\"./kobart_translationese_final\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4312399a-fe55-4efd-b8e3-5ac410e4af50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 입력: UV 보호 기능을 가지고 있습니다. 400mm까지의 모든 유해한 파란색 빛을 100% 필터링합니다. 하지만 착용하기에 매우 편리합니다.\n",
      "✅ 출력: 최대 400mm의 유해 블루라이트 빛을 100% 필터링하여 착용감이 매우 우수합니다.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "\n",
    "model_path = \"kobart_translationese/checkpoint-12037\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_path).to(\"cuda\")\n",
    "model.eval()\n",
    "\n",
    "input_text = \"UV 보호 기능을 가지고 있습니다. 400mm까지의 모든 유해한 파란색 빛을 100% 필터링합니다. 하지만 착용하기에 매우 편리합니다.\"\n",
    "\n",
    "inputs = tokenizer(\n",
    "    input_text,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    max_length=128,\n",
    "    return_token_type_ids=False    \n",
    ").to(\"cuda\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        decoder_start_token_id=tokenizer.eos_token_id,\n",
    "        max_new_tokens=256,\n",
    "        num_beams=5\n",
    "    )\n",
    "\n",
    "result = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "print(\"✅ 입력:\", input_text)\n",
    "print(\"✅ 출력:\", result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "034ce631-e960-45e4-82f2-c3089f9a8d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 1: 철수는 10개의 소를 키운다.\n",
      "완화 1: 철수는 10마리의 소를 키웁니다.\n",
      "\n",
      "입력 2: 그 아이들은 보육원에 의해 보호되었다.\n",
      "완화 2: 그 아이들은 보육원에서 보호되었어요.\n",
      "\n",
      "입력 3: 전통적인 중국 문화에서 결혼에 대한 결정들은 그들의 자녀를 위하는 부모들에 의해 만들어졌다.\n",
      "완화 3: 전통적인 중국 문화에서 결혼은 자녀를 위하는 부모들에 의해 결정됩니다.\n",
      "\n",
      "입력 4: 다음 물음에 대해 답하시오.\n",
      "완화 4: 다음 질문에 답해 주시기 바랍니다.\n",
      "\n",
      "입력 5: 고모는 두 명을 아들을 가지고 있다.\n",
      "완화 5: 고모는 두 아들을 두었어요.\n",
      "\n",
      "입력 6: 나는 미국 여행을 가기 위해 영어를 공부했다.\n",
      "완화 6: 미국 여행을 가려고 영어를 공부하고 있어요.\n",
      "\n",
      "입력 7: 학생들은 공부함에 있어서 불편함이 없었다.\n",
      "완화 7: 학생들은 공부하는 데 불편함이 없습니다.\n",
      "\n",
      "입력 8: 성공하기 위해서는 열심히 노력하지 않으면 안 된다.\n",
      "완화 8: 성공을 위해서는 열심히 노력하지 않으면 안 됩니다.\n",
      "\n",
      "입력 9: 그들 대부분은 좋은 직장을 얻는 데는 새로운 아이디어들과 방법들이 있다는 것을 모른다.\n",
      "완화 9: 대부분은 좋은 직장을 얻는 데 새로운 아이디어와 방법이 있다는 것을 모른다.\n",
      "\n",
      "입력 10: 두 사람이 주점의 2층에서의 살림을 그만두고 교외로 이사하여 노라의 아기 주디와 살고 있다는 이야기를 들었다.\n",
      "완화 10: 2층에 살았던 살림을 그만두고 교외로 이사하여 노라의 아기 주디와 살고 있다는 이야기를 들었습니다.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "\n",
    "model_dir = \"kobart_translationese/checkpoint-12037\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_dir).to(\"cuda\")\n",
    "model.eval()\n",
    "\n",
    "translationese_list = [\n",
    "    '철수는 10개의 소를 키운다.',\n",
    "    '그 아이들은 보육원에 의해 보호되었다.',\n",
    "    '전통적인 중국 문화에서 결혼에 대한 결정들은 그들의 자녀를 위하는 부모들에 의해 만들어졌다.',\n",
    "    '다음 물음에 대해 답하시오.',\n",
    "    '고모는 두 명을 아들을 가지고 있다.',\n",
    "    '나는 미국 여행을 가기 위해 영어를 공부했다.',\n",
    "    '학생들은 공부함에 있어서 불편함이 없었다.',\n",
    "    '성공하기 위해서는 열심히 노력하지 않으면 안 된다.',\n",
    "    '그들 대부분은 좋은 직장을 얻는 데는 새로운 아이디어들과 방법들이 있다는 것을 모른다.',\n",
    "    '두 사람이 주점의 2층에서의 살림을 그만두고 교외로 이사하여 노라의 아기 주디와 살고 있다는 이야기를 들었다.'\n",
    "]\n",
    "\n",
    "for idx, sentence in enumerate(translationese_list, 1):\n",
    "    inputs = tokenizer(\n",
    "        sentence,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    # token_type_ids 키가 있으면 제거\n",
    "    inputs.pop(\"token_type_ids\", None)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            decoder_start_token_id=tokenizer.eos_token_id,\n",
    "            max_new_tokens=128,\n",
    "            num_beams=5,\n",
    "            early_stopping=True\n",
    "        )\n",
    "\n",
    "    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    if \"<extra_id_0>\" in decoded:\n",
    "        result = decoded.split(\"<extra_id_0>\", 1)[-1].strip()\n",
    "    else:\n",
    "        result = decoded.strip()\n",
    "\n",
    "    # 문장 앞의 불필요한 공백/특수문자 제거\n",
    "    result = result.lstrip(\" .>\")\n",
    "\n",
    "    print(f\"입력 {idx}: {sentence}\")\n",
    "    print(f\"완화 {idx}: {result}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c42879c-ec75-46a5-98ea-93315ea11e5d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
